{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representation\n",
    "The collection of documents is named a corpus. The documents being vectors and the collection, the corpus, the vector-space. Each dimension or axis is often called a term or token, signifying that it encompasses both words and characters. The translational mapping between the words an their vector axis is a dictionary. For the subsequent examples we can use the following dictionary and documents:\n",
    "\n",
    "| Term | Mapping |\n",
    "| --- | --- |\n",
    "| where | 0 |\n",
    "| is | 1 |\n",
    "| my | 2 |\n",
    "| money | 3 |\n",
    "| car | 4 |\n",
    "| wallet | 5 |\n",
    "\n",
    "doc1 = \"where is my money\"\n",
    "\n",
    "doc2 = \"i keep my money in my wallet\" \n",
    "\n",
    "doc3 = \"my car is where my money is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"where\": 0, \"is\": 1, \"my\": 2, \"money\":3, \"car\": 4, \"wallet\": 5}\n",
    "\n",
    "doc1 = \"where is my money\"\n",
    "doc2 = \"i keep my money in my wallet\"\n",
    "doc3 = \"my car is where my money is\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts or bags of words\n",
    "\n",
    "Here we represent each document as an array or a tuple. Matter doesn't order word. At least not in this representation. Remember that the model is measured by its usefulness--so let's ignore order and see how far we get.\n",
    "\n",
    "In the case of an *word-count* array we can assume the the position in the array represents the related word dimension. So doc1 could be represented as the array [1,1,1,1,0,0]. There is one occurance of \"where\", so there is a corresponding value of unity at the zeroth position. There is one occurance of \"is\", so there is a value of unity at the first position of the array. Et cetera.\n",
    "\n",
    "Representing the document as a tuple, the format is similar to a vector where the first position indicates the term axis and the second the count along that axis. Rather than wallet=1, a tuple for wallet would be given as (5, 1), since it is in the fifth position. So doc3 would be represented as [(2,2),(4,1),(1,2),(0,1),(3,1)], given that \"is\" and \"my\" occur twice. The benefit of this representation is that is a dense array. Meaning that terms without a count are not required in the description, thus negating the need for lots of zeroes. This is particularly useful where the dictionary may consist of 3000 to 100,000 terms.\n",
    "\n",
    "Note that in the bag-of-words format, \"money my is where\" is identical to doc1. We lose the context in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_array = [0, 0, 2, 1, 0, 1]  # word count vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors & vampires\n",
    "Let's parse the text [Carmilla](https://www.gutenberg.org/ebooks/10007) into a series of bag-of-words tuples and arrays.\n",
    "\n",
    "P.S. Carmilla is a wonderful book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARMILLA\n",
      "\n",
      "J. Sheridan LeFanu\n",
      "\n",
      "1872\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "\n",
      "_Upon a paper attached to the Narrative which follows, Doctor Hesselius\n",
      "has written a rather elaborate note, which he accompanies with a\n",
      "reference to his Essay on the strange subject which the MS. illuminates.\n",
      "\n",
      "This mysterious subject he treats, in tha\n"
     ]
    }
   ],
   "source": [
    "with open('carmilla.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "print(corpus[:300])  # the index is by character in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term/token count: 4368\n"
     ]
    }
   ],
   "source": [
    "# Here we are using a custom method to reveal the basic workings.\n",
    "# Excellent prebuilt methods are available with:\n",
    "# gensim, hugging-face, spaCy, and scikit-learn\n",
    "\n",
    "from utils import dctConstr  # custom class\n",
    "dct = dctConstr()  # initialize the method\n",
    "dct.constructor(corpus)  # build the dictionary of terms\n",
    "print(\"Term/token count:\", len(dct.terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctor Hesselius\n",
      "has written a rather elaborate note, which he accompanies with a\n",
      "reference to his Essay on the strange subject which the MS. illuminates. \n",
      "\n",
      "[(8, 2), (11, 1), (12, 2), (14, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)] \n",
      "\n",
      " [16, 17, 18, 19, 8, 20, 21, 22, 14, 23, 24, 25, 8, 26, 11, 27, 28, 29, 12, 30, 31, 14, 12, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the bag-of-words and word-index array formats\n",
    "# The bag-of-words should be the same length as the number of unique terms.\n",
    "# The word-index array should be the same length as the original words in selection\n",
    "# The word-count array will be the same length as the dictionary >4000 terms, so we  won't print it here\n",
    "\n",
    "sample = corpus[103:257]\n",
    "print(sample, \"\\n\")\n",
    "print(dct(sample), \"\\n\\n\", dct.to_idx(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of terms in the dictionary: do we need all those?\n",
    "\n",
    "What are words? Perhaps too heavy an epistomological question before coffee or alcohol... Thankfully there is a vsauce video to help discuss the [Zipf function](https://www.youtube.com/watch?v=fCn8zs912OE) and some of its implications.\n",
    "\n",
    "Some works offer more \"useful\" information than others for many tasks. If the subject of a conversation is inherant, the term \"I\" is redundant. It is common in many languages for its equivalent to be absent. This is the first of many terms that offer limited information for determining what the document concerns. Generally in knowledge extraction and classification tasks we remove these stop-words as they clutter the vector representations. The common practice is to remove these stop-words as by doing so, we most often improve the accuracy of the model that follows.\n",
    "\n",
    "On the opposite end of Zipf's distribution are the terms that are used very infrequently. Does the inclusion of the term \"parsimonius\" in your dictionary help you improve a model? Perhaps if the object of your model is to separate documents written by academics and everyone else it may be useful... However, if it occurs infrequently within a corpus it represents an outlier in the data. Any model we develop against a corpus will include these infrequent terms. In doing so the model will fit the training data more closely, but therein lies the problem. It will fit the training data and not necessarily the real data.\n",
    "\n",
    "Determining where to cull most frequent and infrequent terms is ultimately a question of the language, the dataset, and the model being used. Build the model based on a best estimate, revise the dictionary, rebuild the model, repeat, and graph the outcomes. If the model fit quickly becomes poor with further reduction, stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'I', 'of', 'a', 'to', 'in', 'was', '\"', 'my', 'her', 'that', 'with', 'you', 'it', 'had', 'me', 'as', 'which', 'she', 'not', 'he', 'is', 'for', 'at', 'have', 'so', 'his', 'on', 'very']\n"
     ]
    }
   ],
   "source": [
    "common_words = [i for i, j in dct.counts.most_common(30)]  # this uses the Counter class\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('⊹', 1), ('CARMILLA', 1), ('J', 1), ('Sheridan', 1), ('LeFanu', 1), ('1872', 1), ('PROLOGUE', 1), ('Upon', 1), ('follows', 1), ('elaborate', 1), ('accompanies', 1), ('reference', 1), ('MS', 1), ('illuminates', 1), ('treats', 1), ('acumen', 1), ('condensation', 1), ('publish', 1), ('\"laity', 1), ('forestall', 1), ('relates', 1), ('due', 1), ('consideration', 1), ('abstain', 1), ('presenting', 1), ('précis', 1), ('reasoning', 1), ('extract', 1), ('describes', 1), ('\"involving', 1)]\n"
     ]
    }
   ],
   "source": [
    "infrequent_words = sorted(dct.counts.items(), key=lambda x: x[1])\n",
    "print(infrequent_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the character '⊹'. This is used where the given word is unknown. So if the *dct* vectorizer translates \"parsimonius\" or \"proactive\" it will assign it to the index zero. We could have used a special word like *UNKN* here, but our method would no longer work with a language like Chinese--where words are not delimited by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before trim number of terms: 4167\n",
      "after trim: 338\n",
      "Term/token count: 338\n"
     ]
    }
   ],
   "source": [
    "from utils import dctConstr\n",
    "dct = dctConstr(stop_words=common_words, ignore_case=True)  # initialize the method\n",
    "dct.constructor(corpus)  # build the dictionary of terms\n",
    "dct.trimmer(min_num=10)  # if occuring less than two times -> remove\n",
    "print(\"Term/token count:\", len(dct.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural delimitation of language\n",
    "So far we have taken an entire corpus and constructed a method for translating it atomically into a simple machine-interpretable vector form. However, we also have the natural units of sentences and paragraphs to work with. \n",
    "\n",
    "Unlike paragraphs, sentences can be hard. Where sentences contain quotations, colons, or semicolons, the period may no longer represent the end of the sentence. If sentences are of interest, libraries such as the [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) are the best place to begin.\n",
    "\n",
    "Here, we will have to separate the corpus into a series of \"documents\" by dividing the corpus by paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we sat here this night, and with candles lighted, were talking over the adventure of the evening.\n"
     ]
    }
   ],
   "source": [
    "from utils import split_by_paragraphs  # this is separating by \\n\\n\n",
    "paragraph_corp = split_by_paragraphs(corpus)\n",
    "print(paragraph_corp[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Lowercase and split by white space, remove\n",
    "carmilla_paragraphs = [dct.to_count_vec(paragraph) for paragraph in paragraph_corp]\n",
    "print(carmilla_paragraphs[100][:100]) # limiting output to first 100 terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frankenstein's monster vs. vampires: a classification task\n",
    "Now that we have a method for translating a complete corpus into a series of machine-interpretable objects, let's apply some machine-learning methods to it.\n",
    "\n",
    "Here we will decide if a given document came from book about vampires or about Frankenstein's monster (let's just say Frankenstein henceforth even though it is incorrect). Let's build the vectorizer from the collated corpora of Frankenstein & Carmilla, trim it, separate each corpus into paragraphs, and vectorize each set of paragraphs. With the two sets of vectorized documents available let's then train a logistic regression model. Subsequently, we can combine our vectorizer and the model to decide if any given document is vampire or frankenstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before trim number of terms: 9116\n",
      "after trim: 513\n"
     ]
    }
   ],
   "source": [
    "# begin by building the vectorizer\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import dctConstr, split_by_paragraphs\n",
    "\n",
    "with open('carmilla.txt', 'r') as f:\n",
    "    carmilla = f.read()\n",
    "with open('frankenstein.txt', 'r') as f:\n",
    "    frankenstein = f.read()\n",
    "    \n",
    "dct = dctConstr(ignore_case=True)\n",
    "dct.constructor(carmilla + frankenstein)  # combine the two strings\n",
    "dct.trimmer(max_num=500, min_num=22)  # remove \"less useful\" terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 790\n",
      "\"yes, a long time. i suffered from this very illness; but i forget all but my pain and weakness, and they were not so bad as are suffered in other diseases.\" \n",
      "\n",
      " i remained motionless. the thunder ceased; but the rain still continued, and the scene was enveloped in an impenetrable darkness. i revolved in my mind the events which i had until now sought to forget: the whole train of my progress toward the creation; the appearance of the works of my own hands at my bedside; its departure. two years had now nearly elapsed since the night on which he first received life; and was this his first crime? alas! i had turned loose into the world a depraved wretch, whose delight was in carnage and misery; had he not murdered my brother?\n"
     ]
    }
   ],
   "source": [
    "# let's split each corpus into paragraphs and vectorize them\n",
    "c_para = split_by_paragraphs(carmilla)\n",
    "f_para = split_by_paragraphs(frankenstein)\n",
    "print(len(c_para), len(f_para))  # be certain that we have an userful number of documents for each\n",
    "\n",
    "c_docs = [dct.to_count_vec(p) for p in c_para]\n",
    "f_docs = [dct.to_count_vec(f) for f in f_para]\n",
    "\n",
    "print(c_para[224], \"\\n\\n\", f_para[222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466 1466 1466\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n"
     ]
    }
   ],
   "source": [
    "# now we can label and randomize their order before we feed them into a model\n",
    "c_labels = [1] * len(c_docs)  # use unity to indicate carmilla -> vampire\n",
    "f_labels = [0] * len(f_docs)  # use zero to indicate frankenstein\n",
    "num_docs = len(c_docs) + len(f_docs)\n",
    "\n",
    "X_data = c_docs + f_docs\n",
    "y_data = c_labels + f_labels\n",
    "\n",
    "print(num_docs, len(X_data), len(y_data))  # double check lengths\n",
    "\n",
    "Z = list(zip(X_data, y_data))  # pair values\n",
    "random.shuffle(Z)\n",
    "X_data, y_data = zip(*Z)\n",
    "\n",
    "print(X_data[0][:10], y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822646657571623"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now use a scikit-learn model\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "clf = LogisticRegression(random_state=42).fit(X_data, y_data)\n",
    "\n",
    "clf.score(X_data, y_data)  # how well does it score against the trained data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(\n",
    "    [dct.to_count_vec(\"yes, a long time. i suffered from this very illness; but i forget all but my pain and weakness, and they were not so bad as are suffered in other diseases.\")]))\n",
    "print(clf.predict(\n",
    "    [dct.to_count_vec(\" i remained motionless. the thunder ceased; but the rain still continued, \")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bram Stoker's Dracula\n",
    "dracula = \"\"\"Poor fellow! He looked desperately sad and broken; even his stalwart\n",
    "manhood seemed to have shrunk somewhat under the strain of his\n",
    "much-tried emotions. He had, I knew, been very genuinely and devotedly\n",
    "attached to his father; and to lose him, and at such a time, was a\n",
    "bitter blow to him. With me he was warm as ever, and to Van Helsing he\n",
    "was sweetly courteous; but I could not help seeing that there was some\n",
    "constraint with him. The Professor noticed it, too, and motioned me to\n",
    "bring him upstairs. I did so, and left him at the door of the room, as I\n",
    "felt he would like to be quite alone with her, but he took my arm and\n",
    "led me in, saying huskily:--\n",
    "\n",
    "\"You loved her too, old fellow; she told me all about it, and there was\n",
    "no friend had a closer place in her heart than you. I don't know how to\n",
    "thank you for all you have done for her. I can't think yet....\"\"\"\n",
    "\n",
    "d_docs = [dct.to_count_vec(d) for d in split_by_paragraphs(dracula)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06541074, 0.93458926],\n",
       "       [0.14465824, 0.85534176]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(d_docs)  # these are the probabilities for both labels: 0=frankenstein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a sample of two, but we have a strong probability for both paragraphs of being labelled as vampire... Apparently, the bag-of-words approach works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "complete_path = \"cloud-run-app/models/literary_monsters.pkl\"\n",
    "with open(complete_path, 'wb') as f:\n",
    "    pickle.dump(clf, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "complete_path = \"cloud-run-app/models/text_vectorizer.pkl\"\n",
    "with open(complete_path, 'wb') as f:\n",
    "    pickle.dump(dct, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Predict a vampire\n",
    "Paste the following URL into your browser and add your own sentence/paragraph after the **?x=** to return a prediction. E.g.:\n",
    "\n",
    "[gcp-cloud-run](https://nlp-demo.fraign.dev/api/vampire?x=i%20remained%20motionless.%20the%20thunder%20ceased;%20but%20the%20rain%20still%20continued)\n",
    "\n",
    "```https://nlp-demo.fraign.dev/api/vampire?x=```\n",
    "\n",
    "This is a Google Cloud Run, which is basically a cloud hosted docker container. The code for this to be run locally is in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beget a vampire: A generative neural network\n",
    "Let's now build a simple recurrent neural network with TensorFlow & Keras. We'll train this network to predict the next individual character/token from the previous characters in the sequence.\n",
    "\n",
    "For our simple model, let's follow [tensorflow introduction to text generation](https://www.tensorflow.org/tutorials/text/text_generation) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\"\"\"\n",
    "If using a GPU you often have to set the memory allocation. Without setting \"growth\",\n",
    "all GPU memory is automatically allocated which can cause it to fallover...\n",
    "\"\"\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARMILLA\n",
      "\n",
      "J. Sheridan LeFanu\n",
      "\n",
      "1872\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "\n",
      "_Upon a paper attached to the Narrative which follows, Doctor Hesselius\n",
      "has written a rather elaborate note, which he accompanies with a\n",
      "reference to his Essay on the strange subject which the MS. illuminates.\n",
      "\n",
      "This mysterious subject he treats, in that Essay, with his usual\n",
      "learning and acumen, and with remarkable directness and condensation. It\n",
      "will form but one volume of the series of that extraordinary man's\n",
      "collected papers.\n",
      "\n",
      "As I publish the case, in this volume, simply to interest the \"laity,\" I\n",
      "shall forestall the intelligent lady, who relates it, in nothing; and\n",
      "after due consideration, I have determined, therefore, to abstain from\n",
      "presenting any précis of the learned Doctor's reasoning, or extract from\n",
      "his statement on a subject which he describes as \"involving, not\n",
      "improbably, some of the profoundest arcana of our dual existence, and\n",
      "its intermediates.\"\n",
      "\n",
      "I was anxious on discovering this paper, to reopen the correspondence\n",
      "comm\n"
     ]
    }
   ],
   "source": [
    "with open('carmilla.txt', 'r') as f:\n",
    "    carmilla = f.read()\n",
    "    \n",
    "print(carmilla[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case they are parsing and making predictions at the character level. In the English case we are limiting our dictionary/vocab to the 27 (& is [and-per-se-and](https://en.wikipedia.org/wiki/Ampersand)) characters, upper & lower, punctuation, and \\n etc. So our NN has a small number of possible outputs. For generalized multi-lingual support, this approach doesn't really perform so elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 25391, 'e': 15246, 'a': 10127, 't': 9989, 'o': 8600, 'n': 8176, 'i': 7467, 's': 7323, 'r': 7228, 'h': 7072, 'd': 5699, 'l': 5105, 'u': 3504, 'm': 3294, '\\n': 3235, 'c': 2981, 'w': 2797, 'y': 2659, ',': 2603, 'f': 2407, 'g': 2328, 'p': 1910, 'b': 1462, '.': 1287, 'v': 1202, 'I': 1041, 'k': 865, '\"': 769, ';': 290, 'T': 229, '-': 213, \"'\": 206, 'x': 202, 'M': 177, 'H': 136, 'S': 135, 'q': 129, 'C': 116, '?': 110, 'A': 107, 'W': 98, 'j': 96, 'B': 85, 'G': 79, '_': 65, 'Y': 59, 'z': 49, 'N': 48, '!': 42, 'P': 37, 'D': 36, 'L': 31, ':': 30, 'K': 26, 'O': 23, 'E': 19, 'V': 19, 'F': 17, 'R': 13, 'X': 8, 'U': 7, 'J': 5, '1': 3, '8': 3, '6': 2, '9': 2, '7': 1, '2': 1, 'é': 1, 'Q': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(carmilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upon a paper attached to the Narrative which follows, Doctor Hesselius has written a rather elaborate note, which he accompanies with a reference to his Essay on the strange subject which the MS. illuminates.\n",
      "\n",
      "This mysterious subject he treats, in that Essay, with his usual learning and acumen, and with remarkable directness and condensation. It will form but one volume of the series of that extraordinary man's collected papers.\n",
      "\n",
      "As I publish the case, in this volume, simply to interest the \"laity,\" I shall forestall the intelligent lady, who relates it, in nothing; and after due consideration, I have determined, therefore, to abstain from presenting any precis of the learned Doctor's reasoning, or extract from his statement on a subject which he describes as \"involving, not improbably, some of the profoundest arcana of our dual existence, and its intermediates.\"\n",
      "\n",
      "I was anxious on discovering this paper, to reopen the correspondence commenced by Doctor Hesselius, so many years before, \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "carmilla = carmilla[49:] # shift to start\n",
    "carmilla = carmilla.replace('é', 'e')\n",
    "carmilla, _ = re.subn(\"[0-9]\", \"Z\", carmilla)\n",
    "carmilla = re.sub(\"(?<=[\\w\\d])\\n(?=[\\w\\d])\", \" \", carmilla)\n",
    "print(carmilla[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 unique characters: ['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(carmilla))\n",
    "print ('{} unique characters: {}'.format(len(vocab), vocab))\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in carmilla])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are constructing data in a pipeline that our RNN can consume. Character sequences are constructed of length 100. The sequences are random cuts of the corpus. We are defining the number of examples per epoch such that we have an idea of when we have run over the complete corpus about once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(carmilla)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the drop_remainder ensures that all batches are the same length\n",
    "# the +1 is to handle input/output. See below.\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model input is a sequence of 100 characters and the output is the character following each character of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch and buffer sizes are determined by the memory available to iterate over the model. Generally, start at a small size to avoid trying to push 10GB to a GPU with only 6GB of memory... TensorFlow errors can be hard to debug.\n",
    "Since the PCIe can be a bottleneck, it is likely quicker to use most of the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model itself\n",
    "Unless you are running this model on a GPU, it is best to stick with the pretrained model.\n",
    "\n",
    "This is a simple sequential model that forms a single pipeline. You can add more layers or change any of those layers to see if you can obtain a more effective or efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size, \n",
    "                          name=\"t_out\")\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16384     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "t_out (Dense)                (64, None, 64)            65600     \n",
      "=================================================================\n",
      "Total params: 4,020,288\n",
      "Trainable params: 4,020,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'storage/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=10,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "23/23 [==============================] - 4s 63ms/step - loss: 3.8226\n",
      "Epoch 2/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 2.8284\n",
      "Epoch 3/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 2.4956\n",
      "Epoch 4/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 2.3294\n",
      "Epoch 5/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 2.2397\n",
      "Epoch 6/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 2.1674\n",
      "Epoch 7/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 2.0991\n",
      "Epoch 8/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 2.0248\n",
      "Epoch 9/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 1.9515\n",
      "Epoch 10/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 1.8827\n",
      "Epoch 11/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 1.8208\n",
      "Epoch 12/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 1.7594\n",
      "Epoch 13/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.7030\n",
      "Epoch 14/200\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 1.6436\n",
      "Epoch 15/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 1.5889\n",
      "Epoch 16/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.5386\n",
      "Epoch 17/200\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 1.4884\n",
      "Epoch 18/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.4425\n",
      "Epoch 19/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.3978\n",
      "Epoch 20/200\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 1.3573\n",
      "Epoch 21/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.3163\n",
      "Epoch 22/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 1.2750\n",
      "Epoch 23/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.2344\n",
      "Epoch 24/200\n",
      "23/23 [==============================] - 2s 72ms/step - loss: 1.1969\n",
      "Epoch 25/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 1.1596\n",
      "Epoch 26/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 1.1172\n",
      "Epoch 27/200\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 1.0787\n",
      "Epoch 28/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 1.0352\n",
      "Epoch 29/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.9964\n",
      "Epoch 30/200\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 0.9546\n",
      "Epoch 31/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.9117\n",
      "Epoch 32/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.8702\n",
      "Epoch 33/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.8255\n",
      "Epoch 34/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.7815\n",
      "Epoch 35/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.7375\n",
      "Epoch 36/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.6948\n",
      "Epoch 37/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.6507\n",
      "Epoch 38/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.6111\n",
      "Epoch 39/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.5681\n",
      "Epoch 40/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.5330\n",
      "Epoch 41/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.4949\n",
      "Epoch 42/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.4593\n",
      "Epoch 43/200\n",
      "23/23 [==============================] - 2s 60ms/step - loss: 0.4274\n",
      "Epoch 44/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3996\n",
      "Epoch 45/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.3748\n",
      "Epoch 46/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.3549\n",
      "Epoch 47/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3368\n",
      "Epoch 48/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.3194\n",
      "Epoch 49/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.3014\n",
      "Epoch 50/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.2896\n",
      "Epoch 51/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2763\n",
      "Epoch 52/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2658\n",
      "Epoch 53/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2589\n",
      "Epoch 54/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2502\n",
      "Epoch 55/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2451\n",
      "Epoch 56/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2392\n",
      "Epoch 57/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2329\n",
      "Epoch 58/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.2264\n",
      "Epoch 59/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2236\n",
      "Epoch 60/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2198\n",
      "Epoch 61/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2132\n",
      "Epoch 62/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.2093\n",
      "Epoch 63/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2068\n",
      "Epoch 64/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2043\n",
      "Epoch 65/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.2002\n",
      "Epoch 66/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1985\n",
      "Epoch 67/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1969\n",
      "Epoch 68/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1954\n",
      "Epoch 69/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1910\n",
      "Epoch 70/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1897\n",
      "Epoch 71/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1879\n",
      "Epoch 72/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1847\n",
      "Epoch 73/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1848\n",
      "Epoch 74/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1781\n",
      "Epoch 75/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1795\n",
      "Epoch 76/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1799\n",
      "Epoch 77/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1776\n",
      "Epoch 78/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1739\n",
      "Epoch 79/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1737\n",
      "Epoch 80/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1709\n",
      "Epoch 81/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1722\n",
      "Epoch 82/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1689\n",
      "Epoch 83/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1674\n",
      "Epoch 84/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1663\n",
      "Epoch 85/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1667\n",
      "Epoch 86/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1635\n",
      "Epoch 87/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1622\n",
      "Epoch 88/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1623\n",
      "Epoch 89/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1613\n",
      "Epoch 90/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1598\n",
      "Epoch 91/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1610\n",
      "Epoch 92/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1598\n",
      "Epoch 93/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1570\n",
      "Epoch 94/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1558\n",
      "Epoch 95/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1563\n",
      "Epoch 96/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1548\n",
      "Epoch 97/200\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 0.1535\n",
      "Epoch 98/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1530\n",
      "Epoch 99/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1515\n",
      "Epoch 100/200\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 0.1518\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1517\n",
      "Epoch 102/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1508\n",
      "Epoch 103/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1478\n",
      "Epoch 104/200\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.1481\n",
      "Epoch 105/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1494\n",
      "Epoch 106/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1481\n",
      "Epoch 107/200\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 0.1458\n",
      "Epoch 108/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1470\n",
      "Epoch 109/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1473\n",
      "Epoch 110/200\n",
      "23/23 [==============================] - 2s 72ms/step - loss: 0.1468\n",
      "Epoch 111/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1456\n",
      "Epoch 112/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1446\n",
      "Epoch 113/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1451\n",
      "Epoch 114/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1437\n",
      "Epoch 115/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1421\n",
      "Epoch 116/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1416\n",
      "Epoch 117/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1417\n",
      "Epoch 118/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1427\n",
      "Epoch 119/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1415\n",
      "Epoch 120/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1408\n",
      "Epoch 121/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1394\n",
      "Epoch 122/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1401\n",
      "Epoch 123/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1399\n",
      "Epoch 124/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1410\n",
      "Epoch 125/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1379\n",
      "Epoch 126/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1374\n",
      "Epoch 127/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1386\n",
      "Epoch 128/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1372\n",
      "Epoch 129/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1351\n",
      "Epoch 130/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1354\n",
      "Epoch 131/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1372\n",
      "Epoch 132/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1359\n",
      "Epoch 133/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1343\n",
      "Epoch 134/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1361\n",
      "Epoch 135/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1350\n",
      "Epoch 136/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1343\n",
      "Epoch 137/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1348\n",
      "Epoch 138/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1351\n",
      "Epoch 139/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1346\n",
      "Epoch 140/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1356\n",
      "Epoch 141/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1306\n",
      "Epoch 142/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1338\n",
      "Epoch 143/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1324\n",
      "Epoch 144/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1338\n",
      "Epoch 145/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1330\n",
      "Epoch 146/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1314\n",
      "Epoch 147/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1316\n",
      "Epoch 148/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1324\n",
      "Epoch 149/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1317\n",
      "Epoch 150/200\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.1303\n",
      "Epoch 151/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1304\n",
      "Epoch 152/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1298\n",
      "Epoch 153/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1307\n",
      "Epoch 154/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1335\n",
      "Epoch 155/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1309\n",
      "Epoch 156/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1302\n",
      "Epoch 157/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1307\n",
      "Epoch 158/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1287\n",
      "Epoch 159/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1310\n",
      "Epoch 160/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1291\n",
      "Epoch 161/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1283\n",
      "Epoch 162/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1300\n",
      "Epoch 163/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1322\n",
      "Epoch 164/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1322\n",
      "Epoch 165/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1327\n",
      "Epoch 166/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1328\n",
      "Epoch 167/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1290\n",
      "Epoch 168/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1283\n",
      "Epoch 169/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1291\n",
      "Epoch 170/200\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.1298\n",
      "Epoch 171/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1316\n",
      "Epoch 172/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1286\n",
      "Epoch 173/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1294\n",
      "Epoch 174/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1292\n",
      "Epoch 175/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1267\n",
      "Epoch 176/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1281\n",
      "Epoch 177/200\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.1266\n",
      "Epoch 178/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1272\n",
      "Epoch 179/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1254\n",
      "Epoch 180/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1263\n",
      "Epoch 181/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1266\n",
      "Epoch 182/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1270\n",
      "Epoch 183/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1266\n",
      "Epoch 184/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1261\n",
      "Epoch 185/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1272\n",
      "Epoch 186/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1253\n",
      "Epoch 187/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1259\n",
      "Epoch 188/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1271\n",
      "Epoch 189/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1270\n",
      "Epoch 190/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1272\n",
      "Epoch 191/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1276\n",
      "Epoch 192/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1274\n",
      "Epoch 193/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1244\n",
      "Epoch 194/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1265\n",
      "Epoch 195/200\n",
      "23/23 [==============================] - 2s 63ms/step - loss: 0.1278\n",
      "Epoch 196/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1247\n",
      "Epoch 197/200\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.1230\n",
      "Epoch 198/200\n",
      "23/23 [==============================] - 2s 62ms/step - loss: 0.1240\n",
      "Epoch 199/200\n",
      "23/23 [==============================] - 2s 61ms/step - loss: 0.1252\n",
      "Epoch 200/200\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.1261\n"
     ]
    }
   ],
   "source": [
    "# NOTE THAT THIS WILL TAKE A LOOOOOOOOOOOOOOOOOOOOOOOOONG TIME ON CPU\n",
    "# ~90 seconds per epoch\n",
    "# Nvidia 1660 ~2s\n",
    "# filesize is around 50MB for this model.\n",
    "\n",
    "history = model.fit(dataset, epochs=200, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(_model, start_string, temperature = 0.001):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  ######################################################################\n",
    "  # Note this idea of \"temperature\" found in the tutorial is misleading. \n",
    "  # We are re-weighting the multinomial distribution, such that the most \n",
    "  # likely character is almost certainly the chosen output.\n",
    "  ######################################################################\n",
    "\n",
    "  # *Low temperatures* results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "  # Here batch size == 1\n",
    "  period_count = 0\n",
    "  _model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = _model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "      # We're stopping early here if there are several periods just to speed up our prediction\n",
    "      if predicted_id == char2idx[\".\"]:\n",
    "            period_count += 1\n",
    "      if period_count >= 5:\n",
    "        break\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild nonsense th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th \n"
     ]
    }
   ],
   "source": [
    "# In this case the model is relatively small, so we can load another into memory safely.\n",
    "modelp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)  # note the batch size\n",
    "modelp.load_weights(\"storage/training_checkpoints/ckpt_1\")\n",
    "print(generate_text(modelp, start_string=u\"wild nonsense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild nonsensed the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said\n"
     ]
    }
   ],
   "source": [
    "modelp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)  # note the batch size\n",
    "modelp.load_weights(\"storage/training_checkpoints/ckpt_10\")\n",
    "print(generate_text(modelp, start_string=u\"wild nonsense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild nonsense apart, deep into my breast. I waked with a scream. The room was lighted by the candle that burnt the men to force the lock. They did so, and we stood, holding our lights aloft, in the doorway, and so that in a little while its site was quite forgotten.\"\n",
      "\n",
      "\"Can you point out where it stone, and accompanied her to the window.\n"
     ]
    }
   ],
   "source": [
    "modelp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)  # note the batch size\n",
    "modelp.load_weights(\"storage/training_checkpoints/ckpt_200\")\n",
    "print(generate_text(modelp, start_string=u\"wild nonsense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild nonsense of strangulation begins?\"\n",
      "\n",
      "\"Yes,\" I answered.\n",
      "\n",
      "\"And--recollect as well as you can--the same person who long ago was called Mircalla, Countess Karnstein. Depapa, but for two opposite reasons. At one time I thought he would laugh at my story, and I could not even point my inquiry, attested the marvelous fact that there was no sign visible that any such thing had happened to me.\n",
      "\n",
      "The housekeeper whispered to the nurse: \"Lay your hand along that hollow her face,' I said; 'and she could not know the shriek at a pace that was perfectly frightful, swerved so as to bring the wheel over the projecting roots, or some other malady, as they often do, he said, knocks at the door, and I really thought, for some seconds,\n",
      "I saw a dark figure near the chimney-piece, but I felt us so like the night you came to us,\" I said.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(modelp, start_string=u\"wild nonsense\", temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fascismall livid mark which all concurred in describing as that induced by the demon's lips,\n",
      "and every symptoms that more than reconcing with manifest delight.\n",
      "\"My dear Baron, how happy I am to see you, I had no hope of meet this evening.\"\n",
      "\n",
      "And then they repeated their directions to me and to Madame, you will be so good as not to let Miss Laura be alone for one moment. That is the only direct to herself, her mother, her history,\n",
      "everything in fact connected with her life, plans, and people to sleep in their coffins, they exhibit all the symptoms that more than reconcing with many swinghis panic to the rest, and after a plunge or two, the whole team broke into a wild gallop together, and I must confess the refined and beautiful face white fleets of water lilies.\n",
      "\n",
      "Over all this the schloss shows its many-windowed front; its towed, caught him in her tiny grasp by the wrist.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(modelp, start_string=u\"fascism\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she fell moonlight with you.\"\n",
      "\n",
      "\"How do you feel now, dear Carmilla? Are you really better?\" I asked.\n",
      "\n",
      "I was very near the turning point from which began the doctor had been broaching, but I think I guess it now.\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "_A Wonderful Likeness_\n",
      "\n",
      "The first thing I recollect after, is Madame standing at the foot of the bed, a little at the dreams. I used to think that evil spirits made dreams, but our doctor told me it is no such things.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(modelp, start_string=u\"she fell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Carmilla Copilot\n",
    "\n",
    "Paste the following URL into your browser and add your own sentence/phrase after the ?x= to return continue the text in the style of Carmilla. E.g.:\n",
    "\n",
    "https://nlp-demo.fraign.dev/api/carmilla?x=\n",
    "\n",
    "This is a Google Cloud Run, which is basically a cloud hosted docker container. The code for this to be run locally is in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful references\n",
    "- [A fun podcast about English words and their origins](http://www.lexitecture.com/)\n",
    "- [A wonderfully informative review of recurrent neural networks](https://arxiv.org/pdf/1506.00019)\n",
    "- [Huggingface](https://huggingface.co/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Natural language toolkit](https://www.nltk.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
