{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representation\n",
    "The collection of documents is named a corpus. The documents being vectors and the collection, the corpus, the vector-space. Each dimension or axis is often called a term or token, signifying that it encompasses both words and characters. The translational mapping between the words an their vector axis is a dictionary. For the subsequent examples we can use the following dictionary and documents:\n",
    "\n",
    "| Term | Mapping |\n",
    "| --- | --- |\n",
    "| where | 0 |\n",
    "| is | 1 |\n",
    "| my | 2 |\n",
    "| money | 3 |\n",
    "| car | 4 |\n",
    "| wallet | 5 |\n",
    "\n",
    "doc1 = \"where is my money\"\n",
    "\n",
    "doc2 = \"i keep my money in my wallet\" \n",
    "\n",
    "doc3 = \"my car is where my money is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"where\": 0, \"is\": 1, \"my\": 2, \"money\":3, \"car\": 4, \"wallet\": 5}\n",
    "\n",
    "doc1 = \"where is my money\"\n",
    "doc2 = \"i keep my money in my wallet\"\n",
    "doc3 = \"my car is where my money is\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts or bags of words\n",
    "\n",
    "Here we represent each document as an array or a tuple. Matter doesn't order word. At least not in this representation. Remember that the model is measured by its usefulness--so let's ignore order and see how far we get.\n",
    "\n",
    "In the case of an *word-count* array we can assume the the position in the array represents the related word dimension. So doc1 could be represented as the array [1,1,1,1,0,0]. There is one occurance of \"where\", so there is a corresponding value of unity at the zeroth position. There is one occurance of \"is\", so there is a value of unity at the first position of the array. Et cetera.\n",
    "\n",
    "Representing the document as a tuple, the format is similar to a vector where the first position indicates the term axis and the second the count along that axis. Rather than wallet=1, a tuple for wallet would be given as (5, 1), since it is in the fifth position. So doc3 would be represented as [(2,2),(4,1),(1,2),(0,1),(3,1)], given that \"is\" and \"my\" occur twice. The benefit of this representation is that is a dense array. Meaning that terms without a count are not required in the description, thus negating the need for lots of zeroes. This is particularly useful where the dictionary may consist of 3000 to 100,000 terms.\n",
    "\n",
    "Note that in the bag-of-words format, \"money my is where\" is identical to doc1. We lose the context in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_array = [0, 0, 2, 1, 0, 1]  # word count vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors & vampires\n",
    "Let's parse the text [Carmilla](https://www.gutenberg.org/ebooks/10007) into a series of bag-of-words tuples and arrays.\n",
    "\n",
    "P.S. Carmilla is a wonderful book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARMILLA\n",
      "\n",
      "J. Sheridan LeFanu\n",
      "\n",
      "1872\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "\n",
      "_Upon a paper attached to the Narrative which follows, Doctor Hesselius\n",
      "has written a rather elaborate note, which he accompanies with a\n",
      "reference to his Essay on the strange subject which the MS. illuminates.\n",
      "\n",
      "This mysterious subject he treats, in tha\n"
     ]
    }
   ],
   "source": [
    "with open('carmilla.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "print(corpus[:300])  # the index is by character in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term/token count: 4368\n"
     ]
    }
   ],
   "source": [
    "# Here we are using a custom method to reveal the basic workings.\n",
    "# Excellent prebuilt methods are available with:\n",
    "# gensim, hugging-face, spaCy, and scikit-learn\n",
    "\n",
    "from utils import dctConstr  # custom class\n",
    "dct = dctConstr()  # initialize the method\n",
    "dct.constructor(corpus)  # build the dictionary of terms\n",
    "print(\"Term/token count:\", len(dct.terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctor Hesselius\n",
      "has written a rather elaborate note, which he accompanies with a\n",
      "reference to his Essay on the strange subject which the MS. illuminates. \n",
      "\n",
      "[(8, 2), (11, 1), (12, 2), (14, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)] \n",
      "\n",
      " [16, 17, 18, 19, 8, 20, 21, 22, 14, 23, 24, 25, 8, 26, 11, 27, 28, 29, 12, 30, 31, 14, 12, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the bag-of-words and word-index array formats\n",
    "# The bag-of-words should be the same length as the number of unique terms.\n",
    "# The word-index array should be the same length as the original words in selection\n",
    "# The word-count array will be the same length as the dictionary >4000 terms, so we  won't print it here\n",
    "\n",
    "sample = corpus[103:257]\n",
    "print(sample, \"\\n\")\n",
    "print(dct(sample), \"\\n\\n\", dct.to_idx(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of terms in the dictionary: do we need all those?\n",
    "\n",
    "What are words? Perhaps too heavy an epistomological question before coffee or alcohol... Thankfully there is a vsauce video to help discuss the [Zipf function](https://www.youtube.com/watch?v=fCn8zs912OE) and some of its implications.\n",
    "\n",
    "Some works offer more \"useful\" information than others for many tasks. If the subject of a conversation is inherant, the term \"I\" is redundant. It is common in many languages for its equivalent to be absent. This is the first of many terms that offer limited information for determining what the document concerns. Generally in knowledge extraction and classification tasks we remove these stop-words as they clutter the vector representations. The common practice is to remove these stop-words as by doing so, we most often improve the accuracy of the model that follows.\n",
    "\n",
    "On the opposite end of Zipf's distribution are the terms that are used very infrequently. Does the inclusion of the term \"parsimonius\" in your dictionary help you improve a model? Perhaps if the object of your model is to separate documents written by academics and everyone else it may be useful... However, if it occurs infrequently within a corpus it represents an outlier in the data. Any model we develop against a corpus will include these infrequent terms. In doing so the model will fit the training data more closely, but therein lies the problem. It will fit the training data and not necessarily the real data.\n",
    "\n",
    "Determining where to cull most frequent and infrequent terms is ultimately a question of the language, the dataset, and the model being used. Build the model based on a best estimate, revise the dictionary, rebuild the model, repeat, and graph the outcomes. If the model fit quickly becomes poor with further reduction, stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'I', 'of', 'a', 'to', 'in', 'was', '\"', 'my', 'her', 'that', 'with', 'you', 'it', 'had', 'me', 'as', 'which', 'she', 'not', 'he', 'is', 'for', 'at', 'have', 'so', 'his', 'on', 'very']\n"
     ]
    }
   ],
   "source": [
    "common_words = [i for i, j in dct.counts.most_common(30)]  # this uses the Counter class\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('⊹', 1), ('CARMILLA', 1), ('J', 1), ('Sheridan', 1), ('LeFanu', 1), ('1872', 1), ('PROLOGUE', 1), ('Upon', 1), ('follows', 1), ('elaborate', 1), ('accompanies', 1), ('reference', 1), ('MS', 1), ('illuminates', 1), ('treats', 1), ('acumen', 1), ('condensation', 1), ('publish', 1), ('\"laity', 1), ('forestall', 1), ('relates', 1), ('due', 1), ('consideration', 1), ('abstain', 1), ('presenting', 1), ('précis', 1), ('reasoning', 1), ('extract', 1), ('describes', 1), ('\"involving', 1)]\n"
     ]
    }
   ],
   "source": [
    "infrequent_words = sorted(dct.counts.items(), key=lambda x: x[1])\n",
    "print(infrequent_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the character '⊹'. This is used where the given word is unknown. So if the *dct* vectorizer translates \"parsimonius\" or \"proactive\" it will assign it to the index zero. We could have used a special word like *UNKN* here, but our method would no longer work with a language like Chinese--where words are not delimited by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before trim number of terms: 4167\n",
      "after trim: 338\n",
      "Term/token count: 338\n"
     ]
    }
   ],
   "source": [
    "from utils import dctConstr\n",
    "dct = dctConstr(stop_words=common_words, ignore_case=True)  # initialize the method\n",
    "dct.constructor(corpus)  # build the dictionary of terms\n",
    "dct.trimmer(min_num=10)  # if occuring less than two times -> remove\n",
    "print(\"Term/token count:\", len(dct.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural delimitation of language\n",
    "So far we have taken an entire corpus and constructed a method for translating it atomically into a simple machine-interpretable vector form. However, we also have the natural units of sentences and paragraphs to work with. \n",
    "\n",
    "Unlike paragraphs, sentences can be hard. Where sentences contain quotations, colons, or semicolons, the period may no longer represent the end of the sentence. If sentences are of interest, libraries such as the [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) are the best place to begin.\n",
    "\n",
    "Here, we will have to separate the corpus into a series of \"documents\" by dividing the corpus by paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we sat here this night, and with candles lighted, were talking over the adventure of the evening.\n"
     ]
    }
   ],
   "source": [
    "from utils import split_by_paragraphs  # this is separating by \\n\\n\n",
    "paragraph_corp = split_by_paragraphs(corpus)\n",
    "print(paragraph_corp[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Lowercase and split by white space, remove\n",
    "carmilla_paragraphs = [dct.to_count_vec(paragraph) for paragraph in paragraph_corp]\n",
    "print(carmilla_paragraphs[100][:100]) # limiting output to first 100 terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frankenstein's monster vs. vampires: a classification task\n",
    "Now that we have a method for translating a complete corpus into a series of machine-interpretable objects, let's apply some machine-learning methods to it.\n",
    "\n",
    "Here we will decide if a given document came from book about vampires or about Frankenstein's monster (let's just say Frankenstein henceforth even though it is incorrect). Let's build the vectorizer from the collated corpora of Frankenstein & Carmilla, trim it, separate each corpus into paragraphs, and vectorize each set of paragraphs. With the two sets of vectorized documents available let's then train a logistic regression model. Subsequently, we can combine our vectorizer and the model to decide if any given document is vampire or frankenstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before trim number of terms: 9116\n",
      "after trim: 513\n"
     ]
    }
   ],
   "source": [
    "# begin by building the vectorizer\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import dctConstr, split_by_paragraphs\n",
    "\n",
    "with open('carmilla.txt', 'r') as f:\n",
    "    carmilla = f.read()\n",
    "with open('frankenstein.txt', 'r') as f:\n",
    "    frankenstein = f.read()\n",
    "    \n",
    "dct = dctConstr(ignore_case=True)\n",
    "dct.constructor(carmilla + frankenstein)  # combine the two strings\n",
    "dct.trimmer(max_num=500, min_num=22)  # remove \"less useful\" terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 790\n",
      "\"yes, a long time. i suffered from this very illness; but i forget all but my pain and weakness, and they were not so bad as are suffered in other diseases.\" \n",
      "\n",
      " i remained motionless. the thunder ceased; but the rain still continued, and the scene was enveloped in an impenetrable darkness. i revolved in my mind the events which i had until now sought to forget: the whole train of my progress toward the creation; the appearance of the works of my own hands at my bedside; its departure. two years had now nearly elapsed since the night on which he first received life; and was this his first crime? alas! i had turned loose into the world a depraved wretch, whose delight was in carnage and misery; had he not murdered my brother?\n"
     ]
    }
   ],
   "source": [
    "# let's split each corpus into paragraphs and vectorize them\n",
    "c_para = split_by_paragraphs(carmilla)\n",
    "f_para = split_by_paragraphs(frankenstein)\n",
    "print(len(c_para), len(f_para))  # be certain that we have an userful number of documents for each\n",
    "\n",
    "c_docs = [dct.to_count_vec(p) for p in c_para]\n",
    "f_docs = [dct.to_count_vec(f) for f in f_para]\n",
    "\n",
    "print(c_para[224], \"\\n\\n\", f_para[222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466 1466 1466\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 1\n"
     ]
    }
   ],
   "source": [
    "# now we can label and randomize their order before we feed them into a model\n",
    "c_labels = [1] * len(c_docs)  # use unity to indicate carmilla -> vampire\n",
    "f_labels = [0] * len(f_docs)  # use zero to indicate frankenstein\n",
    "num_docs = len(c_docs) + len(f_docs)\n",
    "\n",
    "X_data = c_docs + f_docs\n",
    "y_data = c_labels + f_labels\n",
    "\n",
    "print(num_docs, len(X_data), len(y_data))  # double check lengths\n",
    "\n",
    "Z = list(zip(X_data, y_data))  # pair values\n",
    "random.shuffle(Z)\n",
    "X_data, y_data = zip(*Z)\n",
    "\n",
    "print(X_data[0][:10], y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822646657571623"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now use a scikit-learn model\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "clf = LogisticRegression(random_state=42).fit(X_data, y_data)\n",
    "\n",
    "clf.score(X_data, y_data)  # how well does it score against the trained data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(\n",
    "    [dct.to_count_vec(\"yes, a long time. i suffered from this very illness; but i forget all but my pain and weakness, and they were not so bad as are suffered in other diseases.\")]))\n",
    "print(clf.predict(\n",
    "    [dct.to_count_vec(\" i remained motionless. the thunder ceased; but the rain still continued, \")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Bram Stoker's Dracula\n",
    "dracula = \"\"\"Poor fellow! He looked desperately sad and broken; even his stalwart\n",
    "manhood seemed to have shrunk somewhat under the strain of his\n",
    "much-tried emotions. He had, I knew, been very genuinely and devotedly\n",
    "attached to his father; and to lose him, and at such a time, was a\n",
    "bitter blow to him. With me he was warm as ever, and to Van Helsing he\n",
    "was sweetly courteous; but I could not help seeing that there was some\n",
    "constraint with him. The Professor noticed it, too, and motioned me to\n",
    "bring him upstairs. I did so, and left him at the door of the room, as I\n",
    "felt he would like to be quite alone with her, but he took my arm and\n",
    "led me in, saying huskily:--\n",
    "\n",
    "\"You loved her too, old fellow; she told me all about it, and there was\n",
    "no friend had a closer place in her heart than you. I don't know how to\n",
    "thank you for all you have done for her. I can't think yet....\"\"\"\n",
    "\n",
    "d_docs = [dct.to_count_vec(d) for d in split_by_paragraphs(dracula)]\n",
    "print(len(d_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04953485, 0.95046515],\n",
       "       [0.12104853, 0.87895147]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(d_docs)  # these are the probabilities for both labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "complete_path = \"cloud-run-app/models/literary_monsters.pkl\"\n",
    "with open(complete_path, 'wb') as f:\n",
    "    pickle.dump(clf, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "complete_path = \"cloud-run-app/models/text_vectorizer.pkl\"\n",
    "with open(complete_path, 'wb') as f:\n",
    "    pickle.dump(dct, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a sample of two, but we have a strong probability for both paragraphs of being labelled as vampire... Apparently, the bag-of-words approach works well.\n",
    "\n",
    "### Activity 1: Predict a vampire\n",
    "Paste the following URL into your browser and add your own sentence/paragraph after the **?x=** to return a prediction. E.g.:\n",
    "\n",
    "[gcp-cloud-run](https://nlp-demo.fraign.dev/api/vampire?x=i%20remained%20motionless.%20the%20thunder%20ceased;%20but%20the%20rain%20still%20continued)\n",
    "\n",
    "\n",
    "This is a Google Cloud Run, which is basically a cloud hosted docker container. The code for this to be run locally is in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beget a vampire: A generative neural network\n",
    "Let's now build a simple recurrent neural network with TensorFlow & Keras. We'll train this network to predict the next individual character/token from the previous characters in the sequence.\n",
    "\n",
    "For our initial model, let's follow [tensorflow introduction to text generation](https://www.tensorflow.org/tutorials/text/text_generation) tutorial. In this case they are parsing and making predictions at the character level. In the English case we are limiting our dictionary/vocab to the 27 (& is [and-per-se-and](https://en.wikipedia.org/wiki/Ampersand)) characters, upper & lower, punctuation, and \\n etc. So our NN has a small number of possible outputs. For generalized multi-lingual support, this approach doesn't really perform so elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\"\"\"\n",
    "If using a GPU you often have to set the memory allocation. Without setting \"growth\",\n",
    "all GPU memory is automatically allocated which can cause it to fallover.\n",
    "\"\"\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(carmilla))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in carmilla])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are constructing data in a pipeline that our RNN can consume. Character sequences are constructed of length 100. The sequences are random cuts of the corpus. We are defining the number of examples per epoch such that we have an idea of when we have run over the complete corpus about once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(carmilla)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the drop_remainder ensures that all batches are the same length\n",
    "# the +1 is to handle input/output. See below.\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model input is a sequence of 100 characters and the output is the character following each character of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch and buffer sizes are determined by the memory available to iterate over the model. Generally, start at a small size to avoid trying to push 10GB to a GPU with only 6GB of memory... TensorFlow errors can be hard to debug.\n",
    "Since the PCIe can be a bottleneck, it is likely quicker to use most of the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model itself\n",
    "Unless you are running this model on a GPU, it is best to stick with the pretrained model.\n",
    "\n",
    "This is a simple sequential model that forms a single pipeline. You can add more layers or change any of those layers to see if you can obtain a more effective or efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size, \n",
    "                          name=\"t_out\")\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.load_weights(\"storage/training_checkpoints/ckpt_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'storage/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT THIS WILL TAKE A LOOOOOOOOOOOOOOOOOOOOOOOOONG TIME ON CPU\n",
    "# ~90 seconds per epoch\n",
    "# Nvidia 1660 ~2.5s\n",
    "# filesize is around 50MB for this model.\n",
    "\n",
    "history = model.fit(dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(_model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # *Low temperatures* results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "  # this idea of \"temperature\" is misleading. We are changing the multinomial distribution.\n",
    "  temperature = 0.001\n",
    "\n",
    "  # Here batch size == 1\n",
    "  period_count = 0\n",
    "  _model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = _model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "      # We're stopping early here just to speed up our prediction\n",
    "      if predicted_id == char2idx[\".\"]:\n",
    "            period_count += 1\n",
    "      if period_count >= 2:\n",
    "        break\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the model is relatively small, so we can load another into memory safely.\n",
    "modelp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)  # note the batch size\n",
    "modelp.load_weights(\"storage/training_checkpoints/ckpt_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(modelp, start_string=u\"wild nonsense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(modelp, start_string=u\"fascism\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(modelp, start_string=u\"she fell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful references\n",
    "- [A fun podcast about English words and their origins](http://www.lexitecture.com/)\n",
    "- [A wonderfully informative review of recurrent neural networks](https://arxiv.org/pdf/1506.00019)\n",
    "- [Huggingface](https://huggingface.co/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Natural language toolkit](https://www.nltk.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
